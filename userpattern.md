	 __    __       _______. _______ .______         .______      ___   .___________.___________. _______ .______      .__   __. 
	|  |  |  |     /       ||   ____||   _  \        |   _  \    /   \  |           |           ||   ____||   _  \     |  \ |  | 
	|  |  |  |    |   (----`|  |__   |  |_)  |       |  |_)  |  /  ^  \ `---|  |----`---|  |----`|  |__   |  |_)  |    |   \|  | 
	|  |  |  |     \   \    |   __|  |      /        |   ___/  /  /_\  \    |  |        |  |     |   __|  |      /     |  . `  | 
	|  `--'  | .----)   |   |  |____ |  |\  \----.   |  |     /  _____  \   |  |        |  |     |  |____ |  |\  \----.|  |\   | 
	 \______/  |_______/    |_______|| _| `._____|   | _|    /__/     \__\  |__|        |__|     |_______|| _| `._____||__| \__| 
                                                                                                                             
 	  ____    __    ____  __  .___________. __    __                                                                            
  	 \   \  /  \  /   / |  | |           ||  |  |  |                                                                           
   	 \   \/    \/   /  |  | `---|  |----`|  |__|  |                                                                           
     	\            /   |  |     |  |     |   __   |                                                                           
      	\    /\    /    |  |     |  |     |  |  |  |                                                                           
       	\__/  \__/     |__|     |__|     |__|  |__|                                                                           
                                                                                                                             
	  ______   ______   .___  ___. .___  ___.      ___      .__   __.  _______         __       __  .__   __.  _______           
	 /      | /  __  \  |   \/   | |   \/   |     /   \     |  \ |  | |       \       |  |     |  | |  \ |  | |   ____|          
	|  ,----'|  |  |  | |  \  /  | |  \  /  |    /  ^  \    |   \|  | |  .--.  |______|  |     |  | |   \|  | |  |__             
	|  |     |  |  |  | |  |\/|  | |  |\/|  |   /  /_\  \   |  . `  | |  |  |  |______|  |     |  | |  . `  | |   __|            
	|  `----.|  `--'  | |  |  |  | |  |  |  |  /  _____  \  |  |\   | |  '--'  |      |  `----.|  | |  |\   | |  |____           
	 \______| \______/  |__|  |__| |__|  |__| /__/     \__\ |__| \__| |_______/       |_______||__| |__| \__| |_______|   


-> _*Command-Line으로 탐색하는 사용자 패턴*_ <- 

-> Data Engineering Team <-
-> 권정민 <-
-> jeongmin.kwon@sk.com <-

-> 2015-08-27 <-


-> _https://github.com/cojette/UserPatternwithCommandLine_ <-


------------------------------------------------------------------------------

# Outline


- Situation
- What is User Pattern?
- Simple Command-line tools
- Usage
- Conclusion


------------------------------------------------------------------------------

# Situation

- 사용자들의 현황을 간단히 파악해야 할 때
- 신규 서비스 베타 테스트
- 로그 정의가 덜 된 상태에서 로그를 살펴볼 때
- 실험 및 테스트 후 현황 파악
- 한 자리에서 로그를 확인하면서 개발을 빠르게 같이 하고 싶을 때...

 > *Ad-hoc data analysis *

------------------------------------------------------------------------------

# What is User Pattern?

- 강-약-약-중간-약-약-...
- 사용자들이 서비스를 사용하는 방식

- 5W 1H
┍━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑
│    1차 (기본 지표)                                         │      2차 (추가 분석)          	        │  
├───────━━━━━━━━━━━──━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━─━┼─────────━━━━━━━━━━━━━━━━━━━━━━━━━━─━━━━━━┤
│ - Who : 사용자 (user id, ip... )                           |  - Why : 왜 (purpose, target ...)        |
|  - When : 언제 (timestamp)                                 |       					|
|  - Where : 어디서 (page_id, action_id, category ... )      |           				|
|  - What : 무엇을 (login, buy, action... )                  |                                          |
|  - How : 어떻게 (purchase method, device, reference ...)   |                                          |
┕━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙

- 5W로 사용자 패턴을 만들어서 확인하는 것만으로도 이후 추가 분석의 방향을 결정할 수 있음. 




------------------------------------------------------------------------------

# What can you know with User Pattern?

- 전체 사용자 현황
- 기본 지표의 시계열 파악
- 요일별, 일별, 시간별 사용자 수
- 컨텐츠별 사용자 수 및 접속 시간
- 개별 사용자 현황
- 서비스 접속 주기
- 요일별, 일별, 시간별 사용 컨텐츠 및 사용 시간
- 외부 데이터의 적합도 파악


-----------------------------------------------------------------------------------

# Example : 아이템 구매 로그 분석

- 배경: 작은 규모의 게임에서 신규 아이템 몇 개를 임시로 팔기 시작했음. 
 
- 목적: 사용자가 언제 어떤 아이템을 주로 사는 지, 신규 아이템 반응이 있는지 궁금함
  - 테스트로 팔아보고 별 다른 특이한 점이 없고 반응도 없으면 빼려고 임시로 생각 중

- 로그: flat file

	$ head tr_sample.csv
	id,type,user_id,description,del_item_id,gem,bean,recip,recdate
	1963179,13,13465,,200002,0,0,113.32.91.206,2015-07-01 00:00:01.000000
	1963180,9,2910, 산책  ,,2,0,211.187.227.229,2015-07-01 00:00:01.000000
	1963181,9,12341,건강관리,,0,110,202.43.69.142,2015-07-01 00:00:01.000000
	1963182,9,6286, 뮤지컬,,0,1000,218.153.243.28,2015-07-01 00:00:04.000000
	1963183,9,6286, 코미디,,0,700,218.153.243.28,2015-07-01 00:00:04.000000
	1963184,9,6286, 사이다 ,,0,510,218.153.243.28,2015-07-01 00:00:04.000000
	1963185,13,7410,,200003,0,0,223.18.82.32,2015-07-01 00:00:05.000000
	1963186,13,2872,,200006,0,0,115.177.41.196,2015-07-01 00:00:06.000000
	1963187,9,11459, 동네 둘러,,0,-60,110.47.60.204,2015-07-01 00:00:06.000000

 - 세부 내용: 로그 확인, 판매 건수 및 시간별/회원별 판매 현황 등


------------------------------------------------------------------------------

# 5 Steps of Data Analysis

- Data Collection -- 데이터 수집

- Data Cleansing -- 보고자 하는 형태로 데이터 정제

- Data Exploration -- 데이터 탐색

- Data Modeling -- 기본 통계 내용 포함 

- Data interpretation -- 결과 나온 후 해석



------------------------------------------------------------------------------

# 5 Steps of Data Analysis

- Data Collection -- 데이터 수집

- Data Cleansing -- 보고자 하는 형태로 데이터 정제

- Data Exploration -- 데이터 탐색

 > Data Wrangling (간단한 데이터 분석은 거의 해결 가능)
 
 

-------------------------------------------------------------------------------

# Why Command Line?

- Agile

- Augmenting

- Scalable

- Extensible

- Ubiquitous


-------------------------------------------------------------------------------

# Data Wrangling with .csv

- csv (https://ko.wikipedia.org/wiki/CSV_(파일_형식))
 - original : comma-separated values 
 - usual: character-separated values (included tab, other delimeter)
 - basic text log file format 

- csvkit (https://github.com/onyxfish/csvkit)
 - suite of utilities for working with cvs format
 - install: sudo apt-get install 


-------------------------------------------------------------------------------
# Data Collection with Command Line

- copy, mv, ...

- Controlling xlsx : in2csv

    $ in2csv data/log_sample.xlsx > data/log_sample.csv

- Database : sql2csv

    $ sql2csv --db 'sqlite:///data/iris.db' --query 'SELECT \* FROM iris limit 5'

- Internet Download : curl

    $  curl -L -O https://github.com/onyxfish/csvkit/raw/master/examples/realdata/ne_1033_data.xlsx
      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100   158    0   158    0     0     24      0 --:--:--  0:00:06 --:--:--    39
    100 65331  100 65331    0     0   4978      0  0:00:13  0:00:13 --:--:-- 15918


---------------------------------------------------------------------------------
 
# Data Cleansing & Processing with Command Line

- Data check : csvlook + head

	$ head -n 4 tr_sample.csv |csvlook
    	|----------+------+---------+------------------+-------------+-----+------+-----------------+-----------------------------|
    	|  id      | type | user_id |    description   | del_item_id | gem | bean | recip           | recdate                     |
    	|----------+------+---------+------------------+-------------+-----+------+-----------------+-----------------------------|
    	|  1963179 | 13   | 13465   |       	       | 200002      | 0   | 0    | 113.32.91.206   | 2015-07-01 00:00:01.000000  |
    	|  1963180 | 9    | 2910    | 산책             |             | 2   | 0    | 211.187.227.229 | 2015-07-01 00:00:01.000000  |
    	|  1963181 | 9    | 12341   | 건강관리         |             | 0   | 110  | 202.43.69.142   | 2015-07-01 00:00:01.000000  |
    	|  1963182 | 9    | 6286    | 뮤지컬           |             | 0   | 1000 | 218.153.243.28  | 2015-07-01 00:00:04.000000  |
    	|----------+------+---------+------------------+-------------+-----+------+-----------------+-----------------------------|

- Syntax error check: csvclean

	$ csvclean tr_sample.csv
	No errors.



---------------------------------------------------------------------------------
 
# Data Cleansing & Processing with Command Line


- Subset from xlsx : csvcut

	$ csvcut -c 1,3,5,6,9  tr_sample.csv | csvlook |head
	|----------+---------+-------------+-----+-----------------------------|
	|  id      | user_id | del_item_id | gem | recdate                     |
	|----------+---------+-------------+-----+-----------------------------|
	|  1963179 | 13465   | 200002      | 0   | 2015-07-01 00:00:01.000000  |
	|  1963180 | 2910    |             | 2   | 2015-07-01 00:00:01.000000  |
	|  1963181 | 12341   |             | 0   | 2015-07-01 00:00:01.000000  |
	|  1963182 | 6286    |             | 0   | 2015-07-01 00:00:04.000000  |
	|  1963183 | 6286    |             | 0   | 2015-07-01 00:00:04.000000  |
	|  1963184 | 6286    |             | 0   | 2015-07-01 00:00:04.000000  |
	|  1963185 | 7410    | 200003      | 0   | 2015-07-01 00:00:05.000000  |


- Filtering: grep, csvgrep

	$ csvgrep -c 5 -m 200003 tr_sample.csv |head -n 3 | csvlook
	|----------+------+---------+-------------+-------------+-----+------+--------------+-----------------------------|
	|  id      | type | user_id | description | del_item_id | gem | bean | recip        | recdate                     |
	|----------+------+---------+-------------+-------------+-----+------+--------------+-----------------------------|
	|  1963185 | 13   | 7410    |             | 200003      | 0   | 0    | 223.18.82.32 | 2015-07-01 00:00:05.000000  |
	|  1963188 | 13   | 7410    |             | 200003      | 0   | 0    | 223.18.82.32 | 2015-07-01 00:00:06.000000  |
	|----------+------+---------+-------------+-------------+-----+------+--------------+-----------------------------|



------------------------------------------------------------------------------

# Data Exploration with Command Line

- Simple statistics : csvstat

	$ csvstat tr_sample.csv 
  	(중략)
	3. user_id
		<type 'int'>
		Nulls: False
		Min: 231
		Max: 13482
		Sum: 4432677
		Mean: 8865.354
		Median: 10684.0
		Standard Deviation: 4179.08089856
		Unique values: 110
		5 most frequent values:
			10684:	24
			2910:	19
			231:	13
			12778:	12
			11124:	12


	$ csvstat --freq tr_sample.csv
	1. id: { "1963208": 1, "1963209": 1, "1963204": 1, "1963205": 1, "1963206": 1 }
	2. type: { "9": 332, "13": 98, "4": 69, "12": 1 }
	3. user_id: { "10684": 24, "2910": 19, "231": 13, "12778": 12, "11124": 12 }
	4. description: { "] 출석보너스": 78, "] 행운의 룰렛": 75, "상] 건강관리 예": 12, "상] 토크쇼 출연": 11, "보너스": 9 }
	5. del_item_id: { "200003": 44, "200004": 15, "200001": 9, "200002": 9, "200005": 7 }
	6. gem: { "0": 491, "2": 8, "-3": 1 }
	7. bean: { "0": 111, "100": 103, "-100": 26, "200": 25, "50": 24 }
	8. recip: { "211.210.119.47": 24, "211.187.227.229": 19, "111.246.253.42": 16, "220.117.106.184": 13, "180.70.70.178": 13 }


------------------------------------------------------------------------------

# Data Exploration with Command Line


- Exploration with SQL : csvsql
	
		$ csvsql --query "select substr(recdate, 15,2) as hour, count(1) from tr_sample group by substr(recdate, 15,2)" tr_sample.csv | csvlook
		|-------+-----------|
		|  hour | count(1)  |
		|-------+-----------|
		|  00   | 108       |
		|  01   | 122       |
		|  02   | 119       |
		|  03   | 98        |
		|  04   | 53        |
		|-------+-----------|

- Exploration with python : csvpy

	$csvpy tr_sample.csv
	Welcome! "tr_sample.csv" has been loaded in a CSVKitReader object named "reader".
	In [1]: reader.next()
	Out[1]: 
	[u'1963179',
	 u'13',
	 u'13465',
	 u'',
 	u'200002',
 	u'0',
	 u'0',
 	u'113.32.91.206',
 	u'2015-07-01 00:00:01.000000']
	u'a', u'b', u'c']


------------------------------------------------------------------------------


# Data Modeling

- Other Command-line tools: BigML ...

- Programming pipeline : modeling code (python, R.. )

	$ R CMD BATCH  sampleml.R
	$ R -e 'hist(demo.data)' > hist.png
  
  - parallizing modeling code: parallel 

	$ ls data/*.txt | parallel --tag "cat {} | ./user_cluster.R 10"

- Pandashells: shell pipeline with the statistical and visualization tools of the python data-stack
 (https://github.com/robdmc/pandashells)

	$ p.example_data -d tips | p.plot -x total_bill -y tip -s 'o' --title 'Tip Vs Bill' > test.png



---------------------------------------------------------------------------------

# Conclusion


- User Pattern : useful, important and easy 
- Handful command-line tools for simple data analysis 
- Handful of options cover 90% of use cases
- Can speed up ordinary pipelines and code


------------------------------------------------------------------------------

# Reference

## Data Science at the Command Line

- Author: Jeroen Janssens 
- Published by O'Reilly in October 2014
- Overview of all tools on _http://datascienceatthecommandline.com_


## csvkit

## pandashells 

-> Thank you for listening. <-
