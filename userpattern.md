


            __    __       _______. _______ .______         .______      ___   .___________..___________. _______ .______      .__   __.    ____    __    ____  __  .___________. __    __  
           |  |  |  |     /       ||   ____||   _  \        |   _  \    /   \  |           ||           ||   ____||   _  \     |  \ |  |    \   \  /  \  /   / |  | |           ||  |  |  | 
           |  |  |  |    |   (----`|  |__   |  |_)  |       |  |_)  |  /  ^  \ `---|  |----``---|  |----`|  |__   |  |_)  |    |   \|  |     \   \/    \/   /  |  | `---|  |----`|  |__|  | 
           |  |  |  |     \   \    |   __|  |      /        |   ___/  /  /_\  \    |  |         |  |     |   __|  |      /     |  . `  |      \            /   |  |     |  |     |   __   | 
           |  `--'  | .----)   |   |  |____ |  |\  \----.   |  |     /  _____  \   |  |         |  |     |  |____ |  |\  \----.|  |\   |       \    /\    /    |  |     |  |     |  |  |  | 
            \______/  |_______/    |_______|| _| `._____|   | _|    /__/     \__\  |__|         |__|     |_______|| _| `._____||__| \__|        \__/  \__/     |__|     |__|     |__|  |__| 
                                                                                                                                                                                            
             ______   ______   .___  ___. .___  ___.      ___      .__   __.  _______           __       __  .__   __.  _______    .___________.  ______     ______    __           _______.
            /      | /  __  \  |   \/   | |   \/   |     /   \     |  \ |  | |       \         |  |     |  | |  \ |  | |   ____|   |           | /  __  \   /  __  \  |  |         /       |
           |  ,----'|  |  |  | |  \  /  | |  \  /  |    /  ^  \    |   \|  | |  .--.  | ______ |  |     |  | |   \|  | |  |__      `---|  |----`|  |  |  | |  |  |  | |  |        |   (----`
           |  |     |  |  |  | |  |\/|  | |  |\/|  |   /  /_\  \   |  . `  | |  |  |  ||______||  |     |  | |  . `  | |   __|         |  |     |  |  |  | |  |  |  | |  |         \   \    
           |  `----.|  `--'  | |  |  |  | |  |  |  |  /  _____  \  |  |\   | |  '--'  |        |  `----.|  | |  |\   | |  |____        |  |     |  `--'  | |  `--'  | |  `----..----)   |   
            \______| \______/  |__|  |__| |__|  |__| /__/     \__\ |__| \__| |_______/         |_______||__| |__| \__| |_______|       |__|      \______/   \______/  |_______||_______/    
                                                                                                   
                                                                                                                                        
-> _*Command-Line으로 탐색하는 사용자 패턴*_ <- 

-> Data Engineering Team <-
-> 권정민 <-
-> jeongmin.kwon@sk.com <-

-> 2015-08-27 <-


-> _https://github.com/cojette/UserPatternwithCommandLine_ <-


------------------------------------------------------------------------------

# Outline


- Situation
- What is User Pattern?
- Simple Command-line tools
- Usage
- Conclusion


------------------------------------------------------------------------------

# Situation

- 사용자들의 현황을 간단히 파악해야 할 때
- 신규 서비스 베타 테스트
- 로그 정의가 덜 된 상태에서 로그를 살펴볼 때
- 실험 및 테스트 후 현황 파악
- 한 자리에서 로그를 확인하면서 개발을 빠르게 같이 하고 싶을 때...



------------------------------------------------------------------------------

# What is User Pattern?

- 강-약-약-중간-약-약-...
- 사용자들이 서비스를 사용하는 방식

- 5W 1H
┍━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┑
│    1차 (기본 지표)                                                          │      2차 (추가 분석)                       │  
├───────━━━━━━━━━━━──━━━━─━┼──────────━━━━━━┤
│ - Who : 사용자 (user id, ip... )                                         |  - Why : 왜 (purpose, target ...)       |
|  - When : 언제 (timestamp)                                              |                                                     |
|  - Where : 어디서 (page_id, action_id, category ... )           |                                                     |
|  - What : 무엇을 (login, buy, action... )                              |                                                     |
|  - How : 어떻게 (purchase method, device, reference ...)    |                                                     |
┕━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┙

- 5W로 사용자 패턴을 만들어서 확인하는 것만으로도 이후 추가 분석의 방향을 결정할 수 있음. 




------------------------------------------------------------------------------
# What can you know with User Pattern?

- - 전체 사용자 현황
 - 기본 지표의 시계열 파악
 - 요일별, 일별, 시간별 사용자 수
 - 컨텐츠별 사용자 수 및 접속 시간
 - 개별 사용자 현황
 - 서비스 접속 주기
 - 요일별, 일별, 시간별 사용 컨텐츠 및 사용 시간
 - 외부 데이터의 적합도 파악


-----------------------------------------------------------------------------------

# Example : 아이템 구매 로그 분석

 - 배경: 작은 규모의 게임에서 신규 아이템 몇 개를 임시로 팔기 시작했음. 
 - 목적: 사용자가 언제 어떤 아이템을 주로 사는 지, 신규 아이템 반응이 있는지 궁금함
  - 테스트로 팔아보고 별 다른 특이한 점이 없고 반응도 없으면 빼려고 임시로 생각 중
 - 로그: flat file
 - 세부 내용: 로그 확인, 판매 건수 및 시간별/회원별 판매 현황 등


------------------------------------------------------------------------------

# 5 Steps of Data Analysis

- Data Collection -- 데이터 수집

- Data Cleansing -- 보고자 하는 형태로 데이터 정제

- Data Exploration -- 데이터 탐색

- Data Modeling -- 기본 통계 내용 포함 

- Data interpretation -- 결과 나온 후 해석



------------------------------------------------------------------------------
# 5 Steps of Data Analysis

- Data Collection -- 데이터 수집

- Data Cleansing -- 보고자 하는 형태로 데이터 정제

- Data Exploration -- 데이터 탐색

 > Data Wrangling (간단한 데이터 분석은 거의 해결 가능)
 
 

-------------------------------------------------------------------------------
# Why Command Line?

- Agile

- Augmenting

- Scalable

- Extensible

- Ubiquitous


-------------------------------------------------------------------------------

# Data Wrangling with .csv

- csv (https://ko.wikipedia.org/wiki/CSV_(파일_형식))
 - original : comma-separated values 
 - usual: character-separated values (included tab, other delimeter)
 - basic text log file format 

- csvkit (https://github.com/onyxfish/csvkit)
 - suite of utilities for working with cvs format
 - install: sudo apt-get install 


-------------------------------------------------------------------------------
# Data Collection with Command Line

- copy, mv, ...

- Controlling xlsx : in2csv
    $ in2csv data/log_sample.xlsx > data/log_sample.csv

- Database : sql2csv
    $ sql2csv --db 'sqlite:///data/iris.db' --query 'SELECT \* FROM iris limit 5'

- Internet Download : curl
    $  curl -L -O https://github.com/onyxfish/csvkit/raw/master/examples/realdata/ne_1033_data.xlsx
      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100   158    0   158    0     0     24      0 --:--:--  0:00:06 --:--:--    39
    100 65331  100 65331    0     0   4978      0  0:00:13  0:00:13 --:--:-- 15918


---------------------------------------------------------------------------------
# Data Cleansing & Processing with Command Line

- Data check : csvlook + head
    $ head -n 5 tr_sample.csv |csvlook
    |----------+------+---------+------------------+-------------+-----+------+-----------------+-----------------------------|
    |  id      | type | user_id |    description   | del_item_id | gem | bean | recip           | recdate                     |
    |----------+------+---------+------------------+-------------+-----+------+-----------------+-----------------------------|
    |  1963179 | 13   | 13465   |             	   | 200002      | 0   | 0    | 113.32.91.206   | 2015-07-01 00:00:01.000000  |
    |  1963180 | 9    | 2910    | 상] 산책 하는    |             | 2   | 0    | 211.187.227.229 | 2015-07-01 00:00:01.000000  |
    |  1963181 | 9    | 12341   | 상] 건강관리 예  |             | 0   | 110  | 202.43.69.142   | 2015-07-01 00:00:01.000000  |
    |  1963182 | 9    | 6286    | 상] 힙합 뮤지컬  |             | 0   | 1000 | 218.153.243.28  | 2015-07-01 00:00:04.000000  |
    |----------+------+---------+------------------+-------------+-----+------+-----------------+-----------------------------|

- Syntax error check: csvclean
	$ csvclean tr_sample.csv
	No errors.

- Subset from xlsx : csvcut
	$ csvcut -c 1,3,5,6,9  tr_sample.csv | csvlook |head
	|----------+---------+-------------+-----+-----------------------------|
	|  id      | user_id | del_item_id | gem | recdate                     |
	|----------+---------+-------------+-----+-----------------------------|
	|  1963179 | 13465   | 200002      | 0   | 2015-07-01 00:00:01.000000  |
	|  1963180 | 2910    |             | 2   | 2015-07-01 00:00:01.000000  |
	|  1963181 | 12341   |             | 0   | 2015-07-01 00:00:01.000000  |
	|  1963182 | 6286    |             | 0   | 2015-07-01 00:00:04.000000  |
	|  1963183 | 6286    |             | 0   | 2015-07-01 00:00:04.000000  |
	|  1963184 | 6286    |             | 0   | 2015-07-01 00:00:04.000000  |
	|  1963185 | 7410    | 200003      | 0   | 2015-07-01 00:00:05.000000  |


- Filtering: grep, csvgrep
	$ csvgrep -c 5 -m 200003 tr_sample.csv |head -n 3 | csvlook
	|----------+------+---------+-------------+-------------+-----+------+--------------+-----------------------------|
	|  id      | type | user_id | description | del_item_id | gem | bean | recip        | recdate                     |
	|----------+------+---------+-------------+-------------+-----+------+--------------+-----------------------------|
	|  1963185 | 13   | 7410    |             | 200003      | 0   | 0    | 223.18.82.32 | 2015-07-01 00:00:05.000000  |
	|  1963188 | 13   | 7410    |             | 200003      | 0   | 0    | 223.18.82.32 | 2015-07-01 00:00:06.000000  |
	|----------+------+---------+-------------+-------------+-----+------+--------------+-----------------------------|

------------------------------------------------------------------------------
# Data Exploration with Command Line

- Simple statistics : csvstat
	$ csvstat --freq tr_sample.csv
	1. id: { "1963208": 1, "1963209": 1, "1963204": 1, "1963205": 1, "1963206": 1 }
	2. type: { "9": 332, "13": 98, "4": 69, "12": 1 }
	3. user_id: { "10684": 24, "2910": 19, "231": 13, "12778": 12, "11124": 12 }
	4. description: { "] 출석보너스": 78, "] 행운의 룰렛": 75, "상] 건강관리 예": 12, "상] 토크쇼 출연": 11, "보너스": 9 }
	5. del_item_id: { "200003": 44, "200004": 15, "200001": 9, "200002": 9, "200005": 7 }
	6. gem: { "0": 491, "2": 8, "-3": 1 }
	7. bean: { "0": 111, "100": 103, "-100": 26, "200": 25, "50": 24 }
	8. recip: { "211.210.119.47": 24, "211.187.227.229": 19, "111.246.253.42": 16, "220.117.106.184": 13, "180.70.70.178": 13 }
	9. recdate: { "2015-07-01 00:01:39": 11, "2015-07-01 00:02:55": 10, "2015-07-01 00:02:06": 10, "2015-07-01 00:01:44": 7, "2015-07-01 00:03:56": 6 }


- Exploration with SQL : csvsql
	$ csvsql --query "select user_id, max(recdate) as recentdate, sum(gem) as sumgem, sum(bean) as sumbean from tr_sample group by user_id" tr_sample.csv | head |csvlook
	|----------+----------------------------+--------+----------|
	|  user_id | recentdate                 | sumgem | sumbean  |
	|----------+----------------------------+--------+----------|
	|  231     | 2015-07-01 00:03:39.000000 | 0      | 4950     |
	|  233     | 2015-07-01 00:03:19.000000 | 0      | -510     |
	|  410     | 2015-07-01 00:02:28.000000 | 0      | -370     |
	|  456     | 2015-07-01 00:01:41.000000 | 0      | 110      |
	|  707     | 2015-07-01 00:04:20.000000 | 0      | 2290     |
	|  1104    | 2015-07-01 00:01:30.000000 | 0      | 590      |
	|  1223    | 2015-07-01 00:04:18.000000 | 0      | 5450     |
	|  1276    | 2015-07-01 00:01:28.000000 | 0      | 320      |
	|  1695    | 2015-07-01 00:03:44.000000 | 0      | -5220    |
	|----------+----------------------------+--------+----------|


- Exploration with python : csvpy

	$csvpy tr_sample.csv
	Welcome! "tr_sample.csv" has been loaded in a CSVKitReader object named "reader".
	In [1]: reader.next()
	Out[1]: 
	[u'1963179',
	 u'13',
	 u'13465',
	 u'',
 	u'200002',
 	u'0',
	 u'0',
 	u'113.32.91.206',
 	u'2015-07-01 00:00:01.000000']
	u'a', u'b', u'c']


------------------------------------------------------------------------------


# Data Modeling

- Other Command-line tools: BigML ...

- Programming pipeline : modeling code (python, R.. )

	$ R CMD BATCH  sampleml.R
	$ R -e 'hist(demo.data)' > hist.png
  
  - parallizing modeling code: parallel 

	$ ls data/*.txt | parallel --tag "cat {} | ./user_cluster.R 10"

- Pandashells: shell pipeline with the statistical and visualization tools of the python data-stack
 (https://github.com/robdmc/pandashells)

	$ p.example_data -d tips | p.plot -x total_bill -y tip -s 'o' --title 'Tip Vs Bill' > test.png



---------------------------------------------------------------------------------

# Conclusion


- Handful command-line tools for simple data analysis 
- Has a bit of a learning curve
- Handful of options cover 90% of use cases
- Can speed up ordinary pipelines and code


------------------------------------------------------------------------------

# Reference
## Data Science at the Command Line

- Author:
- Published by O'Reilly in October 2014
- Overview of all tools on _http://datascienceatthecommandline.com_



